{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe0b9356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janmi\\AppData\\Local\\Temp\\ipykernel_2112\\2438283841.py:7: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  team_stats_df = pd.read_csv('team_stats_0423_sorted.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Eastern_Accuracy  Western_Accuracy\n",
      "Window_Size                                    \n",
      "1                    0.700483          0.715840\n",
      "2                    0.766086          0.698286\n",
      "3                    0.778638          0.724101\n",
      "4                    0.766300          0.714044\n",
      "5                    0.774048          0.698719\n",
      "7                    0.802327          0.728245\n",
      "The optimal window size is 7 years.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janmi\\AppData\\Local\\Temp\\ipykernel_2112\\2438283841.py:86: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  RF_NetRating = results_df.groupby(['Window_Size']).mean()[['Eastern_Accuracy', 'Western_Accuracy']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "\n",
    "# Load your datasets\n",
    "team_stats_df = pd.read_csv('team_stats_0423_sorted.csv')\n",
    "standings_df = pd.read_csv('NBA_Standings_Ranked_Classes.csv')\n",
    "elo_df = pd.read_csv('elo_ratings_per_year.csv')\n",
    "four_factors_df = pd.read_csv('team_year_avg_four_factors.csv')\n",
    "\n",
    "# Merge the Elo ratings into team stats dataset\n",
    "team_stats_df = pd.merge(team_stats_df, elo_df[['Year', 'Team', 'Elo Rating']], how='left', on=['Year', 'Team'])\n",
    "\n",
    "# Merge the new features into team stats dataset\n",
    "team_stats_df = pd.merge(team_stats_df, four_factors_df[['Year', 'Team', 'eFG%', 'TOV%', 'ORB%', 'FT_Rate']], how='left', on=['Year', 'Team'])\n",
    "\n",
    "# Merge the team stats with the updated standings data\n",
    "merged_data = pd.merge(team_stats_df, standings_df, how='left', on=['Year', 'Team'])\n",
    "\n",
    "# Ensure all necessary columns are available before creating new features\n",
    "required_columns = ['ORtg', 'DRtg', 'W', 'L']\n",
    "missing_columns = [col for col in required_columns if col not in merged_data.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns for feature creation: {missing_columns}\")\n",
    "else:\n",
    "    # Create new features\n",
    "    merged_data['Net_Rating'] = merged_data['ORtg'] - merged_data['DRtg']\n",
    "    merged_data['Win_Loss_Ratio'] = merged_data['W'] / merged_data['L']\n",
    "\n",
    "# Filter the data by year range\n",
    "filtered_data = merged_data[merged_data['Year'] >= 2004]\n",
    "\n",
    "# Select the relevant features for training\n",
    "features = ['Elo Rating', 'eFG%', 'TOV%', 'ORB%', 'FT_Rate', 'Net_Rating', 'Win_Loss_Ratio']\n",
    "\n",
    "# Function to train and evaluate for different windows\n",
    "def train_and_evaluate_yearly(conference_data, start_year, end_year, model):\n",
    "    train_data = conference_data[(conference_data['Year'] >= start_year) & (conference_data['Year'] < end_year)]\n",
    "    test_data = conference_data[conference_data['Year'] == end_year]\n",
    "    \n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data['ranking_class']\n",
    "\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data['ranking_class']\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "results_summary = []\n",
    "\n",
    "# Iterate over the years using different data windows\n",
    "for window_size in [1, 2, 3, 4, 5, 7]:\n",
    "    for year in range(2008, 2023):\n",
    "        eastern_data = filtered_data[filtered_data['Conference'] == 'Eastern Conference']\n",
    "        western_data = filtered_data[filtered_data['Conference'] == 'Western Conference']\n",
    "\n",
    "        # Evaluate for East and west\n",
    "        eastern_accuracy = train_and_evaluate_yearly(eastern_data, year - window_size, year, model)\n",
    "        western_accuracy = train_and_evaluate_yearly(western_data, year - window_size, year, model)\n",
    "        \n",
    "        results_summary.append({\n",
    "            \"Year\": year,\n",
    "            \"Window_Size\": window_size,\n",
    "            \"Model\": \"RandomForest\",\n",
    "            \"Eastern_Accuracy\": eastern_accuracy,\n",
    "            \"Western_Accuracy\": western_accuracy\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "\n",
    "# Group by window size and calculate the mean accuracy\n",
    "RF_NetRating = results_df.groupby(['Window_Size']).mean()[['Eastern_Accuracy', 'Western_Accuracy']]\n",
    "print(RF_NetRating)\n",
    "\n",
    "# perform a final cross-validation on the best window size to validate the findings\n",
    "best_window = RF_NetRating.mean(axis=1).idxmax()\n",
    "print(f\"The optimal window size is {best_window} years.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d6538f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "    Year                                        Best_Params  Accuracy\n",
      "0   2011  {'bootstrap': False, 'max_depth': 10, 'min_sam...  0.767139\n",
      "1   2012  {'bootstrap': True, 'max_depth': 20, 'min_samp...  0.681091\n",
      "2   2013  {'bootstrap': True, 'max_depth': 20, 'min_samp...  0.766884\n",
      "3   2014  {'bootstrap': True, 'max_depth': 10, 'min_samp...  0.666667\n",
      "4   2015  {'bootstrap': False, 'max_depth': 20, 'min_sam...  0.625610\n",
      "5   2016  {'bootstrap': True, 'max_depth': 10, 'min_samp...  0.800000\n",
      "6   2017  {'bootstrap': False, 'max_depth': 10, 'min_sam...  0.866667\n",
      "7   2018  {'bootstrap': False, 'max_depth': 10, 'min_sam...  0.766667\n",
      "8   2019  {'bootstrap': True, 'max_depth': 10, 'min_samp...  0.800000\n",
      "9   2020  {'bootstrap': True, 'max_depth': 10, 'min_samp...  0.833333\n",
      "10  2021  {'bootstrap': True, 'max_depth': 20, 'min_samp...  0.866667\n",
      "11  2022  {'bootstrap': True, 'max_depth': 10, 'min_samp...  0.666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results_summary = []\n",
    "\n",
    "# Iterate over the years using a 7-year window to predict the next year\n",
    "for year in range(2011, 2023):  # Training on 7 years (e.g., 2004-2010 to predict 2011)\n",
    "    train_data = filtered_data[(filtered_data['Year'] >= year - 7) & (filtered_data['Year'] < year)]\n",
    "    test_data = filtered_data[filtered_data['Year'] == year]\n",
    "\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data['ranking_class']\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data['ranking_class']\n",
    "\n",
    "    # Initialize the model\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "    # Fit the GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Use the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results_summary.append({\n",
    "        \"Year\": year,\n",
    "        \"Best_Params\": grid_search.best_params_,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dad0108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "    Year                                        Best_Params  Accuracy\n",
      "0   2011  {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...  0.714736\n",
      "1   2012  {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...  0.771402\n",
      "2   2013  {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...  0.700163\n",
      "3   2014  {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...  0.712602\n",
      "4   2015  {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...  0.681707\n",
      "5   2016  {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...  0.733333\n",
      "6   2017   {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}  0.730894\n",
      "7   2018  {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...  0.767073\n",
      "8   2019   {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}  0.753659\n",
      "9   2020   {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}  0.793673\n",
      "10  2021   {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}  0.766667\n",
      "11  2022   {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}  0.753252\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Store results for Logistic Regression\n",
    "results_summary_lr = []\n",
    "\n",
    "# Define hyperparameter grid for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# Iterate over the years using a 7-year window to predict the next year\n",
    "for year in range(2011, 2023):\n",
    "    train_data = filtered_data[(filtered_data['Year'] >= year - 7) & (filtered_data['Year'] < year)]\n",
    "    test_data = filtered_data[filtered_data['Year'] == year]\n",
    "\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data['ranking_class']\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data['ranking_class']\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LogisticRegression(random_state=42)\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid_lr, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "    # Fit the GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Use the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results_summary_lr.append({\n",
    "        \"Year\": year,\n",
    "        \"Best_Params\": grid_search.best_params_,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df_lr = pd.DataFrame(results_summary_lr)\n",
    "print(results_df_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e92c757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "    Year                                        Best_Params  Accuracy\n",
      "0   2011  {'max_depth': 5, 'min_samples_leaf': 1, 'min_s...  0.833727\n",
      "1   2012  {'max_depth': None, 'min_samples_leaf': 1, 'mi...  0.634525\n",
      "2   2013  {'max_depth': 5, 'min_samples_leaf': 1, 'min_s...  0.666802\n",
      "3   2014  {'max_depth': 10, 'min_samples_leaf': 1, 'min_...  0.700000\n",
      "4   2015  {'max_depth': None, 'min_samples_leaf': 1, 'mi...  0.533333\n",
      "5   2016  {'max_depth': None, 'min_samples_leaf': 1, 'mi...  0.900000\n",
      "6   2017  {'max_depth': None, 'min_samples_leaf': 1, 'mi...  0.666667\n",
      "7   2018  {'max_depth': None, 'min_samples_leaf': 1, 'mi...  0.766667\n",
      "8   2019  {'max_depth': None, 'min_samples_leaf': 1, 'mi...  0.700000\n",
      "9   2020  {'max_depth': None, 'min_samples_leaf': 1, 'mi...  0.764873\n",
      "10  2021  {'max_depth': None, 'min_samples_leaf': 1, 'mi...  0.733333\n",
      "11  2022  {'max_depth': 10, 'min_samples_leaf': 1, 'min_...  0.733333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Store results for Decision Tree\n",
    "results_summary_dt = []\n",
    "\n",
    "# Define hyperparameter grid for Decision Tree\n",
    "param_grid_dt = {\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Iterate over the years using a 7-year window to predict the next year\n",
    "for year in range(2011, 2023):\n",
    "    train_data = filtered_data[(filtered_data['Year'] >= year - 7) & (filtered_data['Year'] < year)]\n",
    "    test_data = filtered_data[filtered_data['Year'] == year]\n",
    "\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data['ranking_class']\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data['ranking_class']\n",
    "\n",
    "    # Initialize the model\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid_dt, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "    # Fit the GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Use the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results_summary_dt.append({\n",
    "        \"Year\": year,\n",
    "        \"Best_Params\": grid_search.best_params_,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df_dt = pd.DataFrame(results_summary_dt)\n",
    "print(results_df_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88e9fd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "    Year                                        Best_Params  Accuracy\n",
      "0   2011  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  0.766745\n",
      "1   2012  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  0.667921\n",
      "2   2013  {'learning_rate': 0.01, 'max_depth': 5, 'n_est...  0.733116\n",
      "3   2014  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  0.666667\n",
      "4   2015  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  0.600000\n",
      "5   2016  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  0.800000\n",
      "6   2017  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  0.800000\n",
      "7   2018  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  0.733333\n",
      "8   2019  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  0.833333\n",
      "9   2020  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  0.799339\n",
      "10  2021  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  0.800000\n",
      "11  2022  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  0.666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Store results for Gradient Boosting\n",
    "results_summary_gb = []\n",
    "\n",
    "# Define hyperparameter grid for Gradient Boosting\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Iterate over the years using a 7-year window to predict the next year\n",
    "for year in range(2011, 2023):\n",
    "    train_data = filtered_data[(filtered_data['Year'] >= year - 7) & (filtered_data['Year'] < year)]\n",
    "    test_data = filtered_data[filtered_data['Year'] == year]\n",
    "\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data['ranking_class']\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data['ranking_class']\n",
    "\n",
    "    # Initialize the model\n",
    "    model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid_gb, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "    # Fit the GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Use the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results_summary_gb.append({\n",
    "        \"Year\": year,\n",
    "        \"Best_Params\": grid_search.best_params_,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for easy viewing\n",
    "results_df_gb = pd.DataFrame(results_summary_gb)\n",
    "print(results_df_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e52c81a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "    Year                                Best_Params  Accuracy\n",
      "0   2011  {'n_neighbors': 3, 'weights': 'distance'}  0.731284\n",
      "1   2012  {'n_neighbors': 3, 'weights': 'distance'}  0.615240\n",
      "2   2013  {'n_neighbors': 3, 'weights': 'distance'}  0.656225\n",
      "3   2014  {'n_neighbors': 7, 'weights': 'distance'}  0.584146\n",
      "4   2015  {'n_neighbors': 3, 'weights': 'distance'}  0.565854\n",
      "5   2016  {'n_neighbors': 3, 'weights': 'distance'}  0.611789\n",
      "6   2017  {'n_neighbors': 3, 'weights': 'distance'}  0.717073\n",
      "7   2018  {'n_neighbors': 3, 'weights': 'distance'}  0.540650\n",
      "8   2019  {'n_neighbors': 3, 'weights': 'distance'}  0.748780\n",
      "9   2020  {'n_neighbors': 3, 'weights': 'distance'}  0.576959\n",
      "10  2021  {'n_neighbors': 5, 'weights': 'distance'}  0.615741\n",
      "11  2022  {'n_neighbors': 7, 'weights': 'distance'}  0.541870\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Store results for K-Nearest Neighbors\n",
    "results_summary_knn = []\n",
    "\n",
    "# Define hyperparameter grid for K-Nearest Neighbors\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# Iterate over the years using a 7-year window to predict the next year\n",
    "for year in range(2011, 2023):\n",
    "    train_data = filtered_data[(filtered_data['Year'] >= year - 7) & (filtered_data['Year'] < year)]\n",
    "    test_data = filtered_data[filtered_data['Year'] == year]\n",
    "\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data['ranking_class']\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data['ranking_class']\n",
    "\n",
    "    # Initialize the model\n",
    "    model = KNeighborsClassifier()\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid_knn, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "    # Fit the GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Use the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results_summary_knn.append({\n",
    "        \"Year\": year,\n",
    "        \"Best_Params\": grid_search.best_params_,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df_knn = pd.DataFrame(results_summary_knn)\n",
    "print(results_df_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769fde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    }
   ],
   "source": [
    "# Define the 7-year window and the prediction year\n",
    "start_year = 2004\n",
    "end_year = 2010\n",
    "prediction_year = 2011\n",
    "\n",
    "# Filter the training data for the 7-year window\n",
    "train_data = filtered_data[(filtered_data['Year'] >= start_year) & (filtered_data['Year'] <= end_year)]\n",
    "# Filter the test data for the prediction year\n",
    "test_data = filtered_data[filtered_data['Year'] == prediction_year]\n",
    "\n",
    "# Ensure that the data is not empty\n",
    "if train_data.empty or test_data.empty:\n",
    "    raise ValueError(f\"No data available for the specified years: {start_year}-{end_year} or {prediction_year}\")\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data['ranking_class']\n",
    "X_test = test_data[features]\n",
    "y_test = test_data['ranking_class']\n",
    "\n",
    "# Initialize the model\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid for Support Vector Classifier\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid_svc, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Use the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Store results\n",
    "results_summary_svc = {\n",
    "    \"Prediction Year\": prediction_year,\n",
    "    \"Best_Params\": grid_search.best_params_,\n",
    "    \"Accuracy\": accuracy\n",
    "}\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df_svc = pd.DataFrame([results_summary_svc])\n",
    "print(results_df_svc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
